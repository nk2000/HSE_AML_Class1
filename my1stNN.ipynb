{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib_utils\n",
    "from IPython.display import HTML, display_html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train, y_train, X_val, y_val, x_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhF3G0RbgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe86d80a978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(x_train[5], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set tensorflow session\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin with logistic regression from the previous assignment to classify some number against others (e.g. zero vs nonzero)\n",
    "\n",
    "## 1.data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [int(y == 0) for y in y_train]\n",
    "Y_test = [int(y == 0) for y in y_test]\n",
    "X_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
    "X_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhF3G0RbgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe86d714b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the result of dimension reducing\n",
    "plt.imshow(X_train[5].reshape(28,28), cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "input_X = tf.placeholder(tf.float32, shape = (None,X_train.shape[1]))\n",
    "input_y = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([X_train.shape[1],1]))\n",
    "b = tf.Variable(0, dtype= tf.float32)\n",
    "\n",
    "# Compute a vector of predictions, resulting shape should be [input_X.shape[0],]\n",
    "# This is 1D, if you have extra dimensions, you can  get rid of them with tf.squeeze .\n",
    "# Don't forget the sigmoid.\n",
    "# predicted_y = <predicted probabilities for input_X>\n",
    "predicted_y = tf.squeeze(tf.sigmoid(tf.matmul(input_X, weights) + b))\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "# tf.reduce_mean is your friend here\n",
    "# loss = <logistic loss (scalar, mean over sample)>\n",
    "#loss = tf.reduce_mean((predicted_y-input_y)**2)\n",
    "loss = -tf.reduce_mean(input_y*tf.log(tf.clip_by_value(predicted_y,1e-10,1.0))+ \n",
    "                      (1-input_y)*tf.log(tf.clip_by_value(1-predicted_y,1e-10,1.0)))\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "# optimizer = <optimizer that minimizes loss>\n",
    "optimizer = tf.train.MomentumOptimizer(0.01, 0.5).minimize(\n",
    "    loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:2.4017\n",
      "train auc: 0.481640795875\n",
      "test auc: 0.465262907824\n",
      "loss at iter 50:1.0437\n",
      "train auc: 0.533447832296\n",
      "test auc: 0.526848613059\n",
      "loss at iter 100:0.8540\n",
      "train auc: 0.61316181285\n",
      "test auc: 0.609065512014\n",
      "loss at iter 150:0.7234\n",
      "train auc: 0.688157819155\n",
      "test auc: 0.685207362324\n",
      "loss at iter 200:0.6231\n",
      "train auc: 0.749631733836\n",
      "test auc: 0.747486311598\n",
      "loss at iter 250:0.5459\n",
      "train auc: 0.796742654031\n",
      "test auc: 0.79539085479\n",
      "loss at iter 300:0.4860\n",
      "train auc: 0.831979172988\n",
      "test auc: 0.831574279379\n",
      "loss at iter 350:0.4390\n",
      "train auc: 0.858365170418\n",
      "test auc: 0.858704805647\n",
      "loss at iter 400:0.4011\n",
      "train auc: 0.878358662635\n",
      "test auc: 0.879304210598\n",
      "loss at iter 450:0.3705\n",
      "train auc: 0.893777860486\n",
      "test auc: 0.895304199285\n"
     ]
    }
   ],
   "source": [
    "# init varables\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    s.run(optimizer, {input_X: X_train, input_y: Y_train})\n",
    "    if i % 50 == 0:\n",
    "        loss_i = s.run(loss, {input_X: X_train, input_y: Y_train})\n",
    "        print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "        print(\"train auc:\", roc_auc_score(Y_train, s.run(predicted_y, {input_X:X_train})))\n",
    "        print(\"test auc:\", roc_auc_score(Y_test, s.run(predicted_y, {input_X:X_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize it to multiclass logistic regression. Either try to remember the week 1 lectures or google it.\n",
    "\n",
    "## 1.data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test,X_train are same as below\n",
    "\n",
    "# do one hot encoding for y\n",
    "Y_train = s.run(tf.one_hot(y_train,depth=10))\n",
    "Y_test = s.run(tf.one_hot(y_test,depth=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Neg_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# num of output\n",
    "output_num = 10\n",
    "\n",
    "# set parameters\n",
    "input_X = tf.placeholder(tf.float32, shape = (None,X_train.shape[1]))\n",
    "input_y = tf.placeholder(tf.float32, shape = (None,output_num))\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([X_train.shape[1],output_num]))\n",
    "b = tf.Variable(tf.random_normal([1,output_num]))\n",
    "\n",
    "predicted_y = tf.nn.softmax(tf.matmul(input_X, weights) + b)\n",
    "print(predicted_y)\n",
    "## define the cross entroy loss\n",
    "loss = -tf.reduce_mean(input_y* tf.log(tf.clip_by_value(predicted_y, 1e-10, 1.0)))\n",
    "print(loss)\n",
    "## difine optimizer\n",
    "optimizer = tf.train.MomentumOptimizer(0.01, 0.5).minimize(\n",
    "    loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:1.2333\n",
      "train auc: 0.49475201078\n",
      "test auc: 0.49528643575\n",
      "loss at iter 1000:0.9637\n",
      "train auc: 0.574702563866\n",
      "test auc: 0.575587623667\n",
      "loss at iter 2000:0.7802\n",
      "train auc: 0.642055298503\n",
      "test auc: 0.643915341373\n",
      "loss at iter 3000:0.6251\n",
      "train auc: 0.698702589431\n",
      "test auc: 0.701454446362\n",
      "loss at iter 4000:0.5172\n",
      "train auc: 0.746743830775\n",
      "test auc: 0.750119997962\n"
     ]
    }
   ],
   "source": [
    "# init varables\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "# number of iteration\n",
    "STEPS = 5000\n",
    "batch_size = 1000\n",
    "dataset_size = X_train.shape[0] #50000\n",
    "for i in range(STEPS):\n",
    "    \n",
    "    # batch gradient descent\n",
    "    start = (i * batch_size) % dataset_size\n",
    "    end = min(start + batch_size, dataset_size)\n",
    "    s.run(optimizer, {input_X: X_train[start:end], input_y: Y_train[start:end]})\n",
    "    \n",
    "    # print the loss in the training process\n",
    "    if i % 1000 == 0:\n",
    "        loss_i = s.run(loss, {input_X: X_train, input_y: Y_train})\n",
    "        print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "        print(\"train auc:\", roc_auc_score(Y_train, s.run(predicted_y, {input_X:X_train})))\n",
    "        print(\"test auc:\", roc_auc_score(Y_test, s.run(predicted_y, {input_X:X_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a hidden layer.\n",
    "\n",
    "## 1. data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = s.run(tf.one_hot(y_train,depth=10))\n",
    "Y_test = s.run(tf.one_hot(y_test,depth=10))\n",
    "X_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
    "X_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax_3:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Neg_4:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "hidden_layer_num = 500\n",
    "\n",
    "\n",
    "input_X = tf.placeholder(tf.float32, shape = (None,X_train.shape[1]))\n",
    "input_y = tf.placeholder(tf.float32, shape = (None,10))\n",
    "\n",
    "weights1 = tf.Variable(tf.random_normal([X_train.shape[1], hidden_layer_num]))\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[hidden_layer_num]))\n",
    "weights2 = tf.Variable(tf.random_normal([hidden_layer_num, 10]))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "# select a Activation function\n",
    "a1 = tf.nn.tanh(tf.matmul(input_X,weights1)+ b1 )\n",
    "\n",
    "predicted_y = tf.nn.softmax(tf.matmul(a1, weights2) + b2)\n",
    "\n",
    "print(predicted_y)\n",
    "## define the cross entroy loss\n",
    "loss = -tf.reduce_mean(input_y* tf.log(tf.clip_by_value(predicted_y, 1e-10, 1.0)))\n",
    "print(loss)\n",
    "## difine optimizer\n",
    "#optimizer = tf.train.MomentumOptimizer(0.01, 0.5).minimize(loss, var_list = [weights1,b1,weights2,b2])\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, var_list = [weights1,b1,weights2,b2])\n",
    "\n",
    "correct = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:1.7290\n",
      "train auc: 0.508643997265\n",
      "test auc: 0.505066108357\n",
      "accuracy: 0.10472\n",
      "loss at iter 1000:0.1177\n",
      "train auc: 0.988654715007\n",
      "test auc: 0.986936992261\n",
      "accuracy: 0.89482\n",
      "loss at iter 2000:0.0672\n",
      "train auc: 0.993462098482\n",
      "test auc: 0.990570018457\n",
      "accuracy: 0.94072\n",
      "loss at iter 3000:0.0479\n",
      "train auc: 0.995228073209\n",
      "test auc: 0.992010056679\n",
      "accuracy: 0.95756\n",
      "loss at iter 4000:0.0355\n",
      "train auc: 0.996266538456\n",
      "test auc: 0.992887460682\n",
      "accuracy: 0.97006\n",
      "loss at iter 5000:0.0287\n",
      "train auc: 0.996864984683\n",
      "test auc: 0.993533453876\n",
      "accuracy: 0.97656\n",
      "loss at iter 6000:0.0237\n",
      "train auc: 0.997239337835\n",
      "test auc: 0.993877489162\n",
      "accuracy: 0.98238\n"
     ]
    }
   ],
   "source": [
    "# init varables\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "# number of iteration\n",
    "STEPS = 10000\n",
    "batch_size = 100\n",
    "dataset_size = X_train.shape[0] #50000\n",
    "\n",
    "loss_list = []\n",
    "for i in range(STEPS):\n",
    "    \n",
    "    # batch gradient descent\n",
    "    start = (i * batch_size) % dataset_size\n",
    "    end = min(start + batch_size, dataset_size)\n",
    "    s.run(optimizer, {input_X: X_train[start:end], input_y: Y_train[start:end]})\n",
    "    \n",
    "    # print the loss in the training process\n",
    "    if i % 1000 == 0:\n",
    "        loss_i = s.run(loss, {input_X: X_train, input_y: Y_train})\n",
    "        loss_list.append(loss_i)\n",
    "        print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "        print(\"train auc:\", roc_auc_score(Y_train, s.run(predicted_y, {input_X:X_train})))\n",
    "        print(\"test auc:\", roc_auc_score(Y_test, s.run(predicted_y, {input_X:X_test})))\n",
    "        print(\"accuracy:\", s.run(accuracy, {input_X:X_train, input_y: Y_train}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
